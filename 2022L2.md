# Preview 1 Optimization攻略

![](assets/20250722_234803_image.png)

## 训练集上损失大

### 模型本身的偏差：模型弹性不够或忽略了重要feature

“模型能表示的函数”这个集合太小，不包括想要的函数

增加模型的弹性就是增加模型的复杂度，就是增加参数的数量与结构的复杂度

> 增加模型输入的feature（即增加第一层的参数数量、让模型变得更胖、让模型能表示更加复杂的函数）

> 增加模型的Layer与neurons

### Optimization不足

卡在 local minimum，而非模型弹性不足
![](assets/20250723_002042_image.png)

> 判断是模型本身的问题还是Optimization不足，可以通过一个较小、浅的模型，
> 这种模型Optimization难度较低，他的Optimization比较容易
> 这时候如果大的模型的loss比小的更大，那就说明是Optimization不足
> 具体solution见下一节课

## 训练集loss小了，但是测试集loss大

### 过拟合 overfitting

> 增加训练集
> Data augmentation

![](assets/20250729_135310_image.png)

### 错配 mismatch

训练资料与测试资料分布不同

# Preview 2 Optimization失败了怎么办

![](assets/20250729_213623_image.png)

optimization失败，也就是梯度为0
分为2种情况，一种是saddle point，一种是local minima
由于实际的训练是在高维空间进行的，过半的情况都是saddle point

# Preview 3 batch & momentum

## batch：size大一点还是小一点好？

### 时间开销

由于GPU并行计算的存在，一次update的耗时与batch size并不是线性的，在一定范围内，随着batch size增大，update耗时几乎不变。

而对于整个epoch，大的batch速度更快

![](assets/20250730_163652_image.png)

### 优化性能

在训练集与验证集中，小一点的batch size效果更好
也就是带有一点噪声的loss居然能达到更好的效果！

![](assets/20250730_164213_image.png)

多样化的loss更容易找到optimization的方向
在一个batch中卡主的时候，另一个batch能够使他更让他容易逃出来

![](assets/20250730_171033_image.png)

小的batch还能避免overfitting：

![](assets/20250730_182753_image.png)

为什么？

- “盆地”形状的minima能更好的适用于testing set，而“深井”形式的最小值对参数取值过于敏感，容易导致overfitting
  所以我们更喜欢的minima是“盆地”形状的，不喜欢“深井”类型的

  ![](assets/20250730_182928_image.png)
- 而small batch本来就有类似于testing data的比较不一样的分布，天生就更容易走到盆地而非深井

![](assets/20250730_183912_image.png)

## Momentum

每次迭代不只是$学习率\cdot梯度$，而是加上前一步的加权

# Lesson 2 宝可梦、数码宝贝分类器

> 得开始加速喽~
